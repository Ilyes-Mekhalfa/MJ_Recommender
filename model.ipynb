{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4205c54",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d46001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation # this smarter  then using countVectorizer or tfidfvectorizer \n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d945e338",
   "metadata": {},
   "source": [
    "# Loading the datasets from the same directory\n",
    "the dataset is generated waitin for the Ministry to give us real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e418cdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (16, 12)\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "CONFIG = {\n",
    "    \"base_model\": \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 16,\n",
    "    \"warmup_steps\": 100,\n",
    "    \"evaluation_steps\": 200,\n",
    "    \"output_path\": \"./fine_tuned_youth_events_model\",\n",
    "    \"train_split\": 0.8\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINE-TUNING SENTENCE TRANSFORMER FOR YOUTH EVENTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìã Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# === LOAD DATA ===\n",
    "print(\"\\nüìÇ Loading dataset...\")\n",
    "users_df = pd.read_csv('users_1000.csv')\n",
    "users_df['interests'] = users_df['interests'].apply(eval)\n",
    "\n",
    "events_df = pd.read_csv('events_1000.csv')\n",
    "events_df['category'] = events_df['category'].apply(eval)\n",
    "\n",
    "annexes_df = pd.read_csv('annexes_48.csv')\n",
    "\n",
    "# Use improved ground truth\n",
    "try:\n",
    "    with open('ground_truth_improved.json', 'r', encoding='utf-8') as f:\n",
    "        ground_truth = json.load(f)\n",
    "        print(\"   ‚úì Using improved ground truth\")\n",
    "except FileNotFoundError:\n",
    "    with open('ground_truth_1000.json', 'r', encoding='utf-8') as f:\n",
    "        ground_truth = json.load(f)\n",
    "        print(\"   ‚ö†Ô∏è  Using original ground truth (run diagnose script first!)\")\n",
    "\n",
    "ground_truth = {int(k): v for k, v in ground_truth.items()}\n",
    "\n",
    "events_df = events_df.merge(\n",
    "    annexes_df[[\"annex_id\", \"wilaya\", \"annex_name\"]],\n",
    "    on=\"annex_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(f\"   ‚úì Loaded {len(users_df)} users, {len(events_df)} events\")\n",
    "\n",
    "# === CREATE TRAINING EXAMPLES ===\n",
    "print(\"\\nüî® Creating training examples...\")\n",
    "\n",
    "def create_training_examples(users_df, events_df, ground_truth, num_negative_samples=2):\n",
    "    \"\"\"\n",
    "    Create training pairs:\n",
    "    - Positive: (user_interests, relevant_event) with label=1.0\n",
    "    - Negative: (user_interests, irrelevant_event) with label=0.0\n",
    "    - Medium: (user_interests, partially_relevant_event) with label=0.3-0.7\n",
    "    \"\"\"\n",
    "    training_examples = []\n",
    "    \n",
    "    for idx, user in users_df.iterrows():\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"   Processing user {idx}/{len(users_df)}...\")\n",
    "        \n",
    "        user_id = user[\"id\"]\n",
    "        user_profile = \" \".join(user[\"interests\"])\n",
    "        user_interests_set = set(user[\"interests\"])\n",
    "        \n",
    "        true_events = ground_truth.get(user_id, [])\n",
    "        if not true_events:\n",
    "            continue\n",
    "        \n",
    "        # POSITIVE EXAMPLES: Ground truth events\n",
    "        for event_id in true_events[:3]:\n",
    "            event = events_df[events_df['id'] == event_id]\n",
    "            if len(event) == 0:\n",
    "                continue\n",
    "            event = event.iloc[0]\n",
    "            \n",
    "            event_text = f\"{event['title']} {' '.join(event['category'])}\"\n",
    "            training_examples.append(\n",
    "                InputExample(texts=[user_profile, event_text], label=1.0)\n",
    "            )\n",
    "        \n",
    "        # NEGATIVE EXAMPLES: Random events NOT in ground truth\n",
    "        all_event_ids = set(events_df['id'].tolist())\n",
    "        negative_event_ids = list(all_event_ids - set(true_events))\n",
    "        \n",
    "        for _ in range(num_negative_samples):\n",
    "            neg_event_id = random.choice(negative_event_ids)\n",
    "            neg_event = events_df[events_df['id'] == neg_event_id].iloc[0]\n",
    "            \n",
    "            # Check if truly negative (no interest overlap)\n",
    "            event_cats = set(neg_event['category'])\n",
    "            overlap = len(user_interests_set & event_cats)\n",
    "            \n",
    "            if overlap == 0:\n",
    "                neg_event_text = f\"{neg_event['title']} {' '.join(neg_event['category'])}\"\n",
    "                training_examples.append(\n",
    "                    InputExample(texts=[user_profile, neg_event_text], label=0.0)\n",
    "                )\n",
    "        \n",
    "        # MEDIUM EXAMPLES: Partial overlap\n",
    "        for event_id in negative_event_ids[:2]:\n",
    "            event = events_df[events_df['id'] == event_id]\n",
    "            if len(event) == 0:\n",
    "                continue\n",
    "            event = event.iloc[0]\n",
    "            \n",
    "            event_cats = set(event['category'])\n",
    "            overlap_ratio = len(user_interests_set & event_cats) / len(user_interests_set) if user_interests_set else 0\n",
    "            \n",
    "            if 0 < overlap_ratio < 0.6:\n",
    "                event_text = f\"{event['title']} {' '.join(event['category'])}\"\n",
    "                training_examples.append(\n",
    "                    InputExample(texts=[user_profile, event_text], label=float(overlap_ratio))\n",
    "                )\n",
    "    \n",
    "    return training_examples\n",
    "\n",
    "training_examples = create_training_examples(users_df, events_df, ground_truth)\n",
    "print(f\"   ‚úì Created {len(training_examples)} training examples\")\n",
    "\n",
    "# Split train/validation\n",
    "random.shuffle(training_examples)\n",
    "split_idx = int(CONFIG['train_split'] * len(training_examples))\n",
    "train_examples = training_examples[:split_idx]\n",
    "val_examples = training_examples[split_idx:]\n",
    "\n",
    "print(f\"   ‚úì Training set: {len(train_examples)}\")\n",
    "print(f\"   ‚úì Validation set: {len(val_examples)}\")\n",
    "\n",
    "# === EVALUATION FUNCTIONS ===\n",
    "def evaluate_recommendations(model_path, test_users, events_df, ground_truth, model_name=\"Model\"):\n",
    "    \"\"\"Evaluate recommendation performance\"\"\"\n",
    "    model = SentenceTransformer(model_path)\n",
    "    \n",
    "    # Create event embeddings\n",
    "    event_descriptions = (\n",
    "        events_df[\"title\"] + \" \" + \n",
    "        events_df[\"category\"].apply(lambda x: \" \".join(x) + \" \" + \" \".join(x))\n",
    "    ).str.lower().tolist()\n",
    "    \n",
    "    event_embeddings = model.encode(event_descriptions, show_progress_bar=False)\n",
    "    \n",
    "    recommendations = {}\n",
    "    \n",
    "    for idx, user in test_users.iterrows():\n",
    "        user_profile = \" \".join(user[\"interests\"]) + \" \" + \" \".join(user[\"interests\"])\n",
    "        user_embedding = model.encode([user_profile.lower()])\n",
    "        \n",
    "        semantic_sim = cosine_similarity(user_embedding, event_embeddings).flatten()\n",
    "        \n",
    "        location_score = events_df[\"wilaya\"].apply(\n",
    "            lambda w: 1 if w == user[\"wilaya\"] else 0\n",
    "        ).values\n",
    "        \n",
    "        user_interests = set(user[\"interests\"])\n",
    "        overlap_scores = events_df[\"category\"].apply(\n",
    "            lambda cats: len(set(cats) & user_interests) / len(user_interests)\n",
    "        ).values\n",
    "        \n",
    "        final_score = 0.5 * semantic_sim + 0.2 * location_score + 0.3 * overlap_scores\n",
    "        \n",
    "        top_indices = final_score.argsort()[-5:][::-1]\n",
    "        recommendations[user[\"id\"]] = events_df.iloc[top_indices][\"id\"].tolist()\n",
    "    \n",
    "\n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for user_id, pred in recommendations.items():\n",
    "        true = ground_truth.get(user_id, [])\n",
    "        if not true:\n",
    "            continue\n",
    "        \n",
    "        correct = len(set(pred) & set(true))\n",
    "        precision = correct / len(pred) if pred else 0\n",
    "        recall = correct / len(true) if true else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    f1 = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"precision\": avg_precision,\n",
    "        \"recall\": avg_recall,\n",
    "        \"f1\": f1,\n",
    "        \"precision_scores\": precisions,\n",
    "        \"recall_scores\": recalls\n",
    "    }\n",
    "\n",
    "# === BASELINE EVALUATION ===\n",
    "print(\"\\nüìä Evaluating BASELINE model (before training)...\")\n",
    "test_users = users_df.sample(200, random_state=42)\n",
    "baseline_metrics = evaluate_recommendations(\n",
    "    CONFIG['base_model'], \n",
    "    test_users, \n",
    "    events_df, \n",
    "    ground_truth,\n",
    "    \"Baseline\"\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"   Precision: {baseline_metrics['precision']:.3f}\")\n",
    "print(f\"   Recall:    {baseline_metrics['recall']:.3f}\")\n",
    "print(f\"   F1-Score:  {baseline_metrics['f1']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681fe39c",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a4cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === LOAD MODEL AND PREPARE TRAINING ===\n",
    "print(\"\\nü§ñ Loading base model...\")\n",
    "model = SentenceTransformer(CONFIG['base_model'])\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=CONFIG['batch_size'])\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# Prepare validation evaluator\n",
    "val_sentences1 = [ex.texts[0] for ex in val_examples[:200]]\n",
    "val_sentences2 = [ex.texts[1] for ex in val_examples[:200]]\n",
    "val_scores = [ex.label for ex in val_examples[:200]]\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator(val_sentences1, val_sentences2, val_scores)\n",
    "\n",
    "# === TRAINING ===\n",
    "print(\"\\nüöÄ Starting training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "training_start = time.time()\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluator,\n",
    "    epochs=CONFIG['epochs'],\n",
    "    warmup_steps=CONFIG['warmup_steps'],\n",
    "    output_path=CONFIG['output_path'],\n",
    "    evaluation_steps=CONFIG['evaluation_steps'],\n",
    "    save_best_model=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "training_time = time.time() - training_start\n",
    "print(f\"\\n‚úÖ Training complete in {training_time:.2f} seconds!\")\n",
    "print(f\"‚úÖ Model saved to: {CONFIG['output_path']}\")\n",
    "\n",
    "# === POST-TRAINING EVALUATION ===\n",
    "print(\"\\nüìä Evaluating FINE-TUNED model (after training)...\")\n",
    "finetuned_metrics = evaluate_recommendations(\n",
    "    CONFIG['output_path'], \n",
    "    test_users, \n",
    "    events_df, \n",
    "    ground_truth,\n",
    "    \"Fine-tuned\"\n",
    ")\n",
    "\n",
    "print(f\"   Precision: {finetuned_metrics['precision']:.3f}\")\n",
    "print(f\"   Recall:    {finetuned_metrics['recall']:.3f}\")\n",
    "print(f\"   F1-Score:  {finetuned_metrics['f1']:.3f}\")\n",
    "\n",
    "# === CALCULATE IMPROVEMENTS ===\n",
    "precision_improvement = (finetuned_metrics['precision'] - baseline_metrics['precision']) / baseline_metrics['precision'] * 100\n",
    "recall_improvement = (finetuned_metrics['recall'] - baseline_metrics['recall']) / baseline_metrics['recall'] * 100\n",
    "f1_improvement = (finetuned_metrics['f1'] - baseline_metrics['f1']) / baseline_metrics['f1'] * 100\n",
    "\n",
    "print(\"\\nüìà IMPROVEMENT:\")\n",
    "print(f\"   Precision: {precision_improvement:+.1f}%\")\n",
    "print(f\"   Recall:    {recall_improvement:+.1f}%\")\n",
    "print(f\"   F1-Score:  {f1_improvement:+.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bfe49a",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028513b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: Before vs After Metrics\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "baseline_vals = [baseline_metrics['precision'], baseline_metrics['recall'], baseline_metrics['f1']]\n",
    "finetuned_vals = [finetuned_metrics['precision'], finetuned_metrics['recall'], finetuned_metrics['f1']]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, baseline_vals, width, label='Before Training', color='#e74c3c', alpha=0.8)\n",
    "bars2 = ax1.bar(x + width/2, finetuned_vals, width, label='After Training', color='#2ecc71', alpha=0.8)\n",
    "\n",
    "ax1.set_ylabel('Score', fontweight='bold', fontsize=11)\n",
    "ax1.set_title('Model Performance: Before vs After Training', fontweight='bold', fontsize=12)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(metrics)\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 2: Improvement Percentage\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "improvements = [precision_improvement, recall_improvement, f1_improvement]\n",
    "colors = ['#2ecc71' if imp > 0 else '#e74c3c' for imp in improvements]\n",
    "\n",
    "bars = ax2.barh(metrics, improvements, color=colors, alpha=0.8)\n",
    "ax2.set_xlabel('Improvement (%)', fontweight='bold', fontsize=11)\n",
    "ax2.set_title('Performance Improvement After Training', fontweight='bold', fontsize=12)\n",
    "ax2.axvline(0, color='black', linewidth=0.8)\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "for i, (bar, val) in enumerate(zip(bars, improvements)):\n",
    "    ax2.text(val + 1, i, f'{val:+.1f}%', va='center', fontsize=10)\n",
    "\n",
    "# Plot 3: Precision Distribution (Before)\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.hist(baseline_metrics['precision_scores'], bins=20, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "ax3.axvline(baseline_metrics['precision'], color='darkred', linestyle='--', linewidth=2, \n",
    "            label=f'Mean: {baseline_metrics[\"precision\"]:.3f}')\n",
    "ax3.set_xlabel('Precision@5', fontweight='bold', fontsize=11)\n",
    "ax3.set_ylabel('Number of Users', fontweight='bold', fontsize=11)\n",
    "ax3.set_title('Precision Distribution (Before Training)', fontweight='bold', fontsize=12)\n",
    "ax3.legend()\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# Plot 4: Precision Distribution (After)\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "ax4.hist(finetuned_metrics['precision_scores'], bins=20, color='#2ecc71', alpha=0.7, edgecolor='black')\n",
    "ax4.axvline(finetuned_metrics['precision'], color='darkgreen', linestyle='--', linewidth=2,\n",
    "            label=f'Mean: {finetuned_metrics[\"precision\"]:.3f}')\n",
    "ax4.set_xlabel('Precision@5', fontweight='bold', fontsize=11)\n",
    "ax4.set_ylabel('Number of Users', fontweight='bold', fontsize=11)\n",
    "ax4.set_title('Precision Distribution (After Training)', fontweight='bold', fontsize=12)\n",
    "ax4.legend()\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "# Plot 5: Recall Comparison\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "ax5.hist(baseline_metrics['recall_scores'], bins=20, color='#e74c3c', alpha=0.5, \n",
    "         label=f'Before: {baseline_metrics[\"recall\"]:.3f}', edgecolor='black')\n",
    "ax5.hist(finetuned_metrics['recall_scores'], bins=20, color='#2ecc71', alpha=0.5,\n",
    "         label=f'After: {finetuned_metrics[\"recall\"]:.3f}', edgecolor='black')\n",
    "ax5.set_xlabel('Recall@5', fontweight='bold', fontsize=11)\n",
    "ax5.set_ylabel('Number of Users', fontweight='bold', fontsize=11)\n",
    "ax5.set_title('Recall Distribution Comparison', fontweight='bold', fontsize=12)\n",
    "ax5.legend()\n",
    "ax5.grid(alpha=0.3)\n",
    "\n",
    "# Plot 6: Training Summary Stats\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "ax6.axis('off')\n",
    "summary_text = f\"\"\"\n",
    "TRAINING SUMMARY\n",
    "{'='*40}\n",
    "\n",
    "Dataset:\n",
    "  ‚Ä¢ Users: {len(users_df):,}\n",
    "  ‚Ä¢ Events: {len(events_df):,}\n",
    "  ‚Ä¢ Training examples: {len(train_examples):,}\n",
    "  ‚Ä¢ Validation examples: {len(val_examples):,}\n",
    "\n",
    "Training Configuration:\n",
    "  ‚Ä¢ Epochs: {CONFIG['epochs']}\n",
    "  ‚Ä¢ Batch size: {CONFIG['batch_size']}\n",
    "  ‚Ä¢ Training time: {training_time:.1f}s\n",
    "\n",
    "Results:\n",
    "  ‚Ä¢ Precision: {baseline_metrics['precision']:.3f} ‚Üí {finetuned_metrics['precision']:.3f}\n",
    "  ‚Ä¢ Recall: {baseline_metrics['recall']:.3f} ‚Üí {finetuned_metrics['recall']:.3f}\n",
    "  ‚Ä¢ F1-Score: {baseline_metrics['f1']:.3f} ‚Üí {finetuned_metrics['f1']:.3f}\n",
    "\n",
    "Status: {'‚úÖ SUCCESS' if finetuned_metrics['precision'] > baseline_metrics['precision'] else '‚ö†Ô∏è  CHECK NEEDED'}\n",
    "\"\"\"\n",
    "ax6.text(0.1, 0.5, summary_text, fontsize=10, family='monospace',\n",
    "         verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "# Plot 7: Accuracy Over Time (simulated)\n",
    "ax7 = fig.add_subplot(gs[2, :])\n",
    "epochs_range = range(1, CONFIG['epochs'] + 1)\n",
    "# Simulate training curve\n",
    "base_prec = baseline_metrics['precision']\n",
    "final_prec = finetuned_metrics['precision']\n",
    "simulated_precision = [base_prec + (final_prec - base_prec) * (1 - np.exp(-2*e/CONFIG['epochs'])) \n",
    "                       for e in epochs_range]\n",
    "simulated_recall = [baseline_metrics['recall'] + \n",
    "                    (finetuned_metrics['recall'] - baseline_metrics['recall']) * \n",
    "                    (1 - np.exp(-2*e/CONFIG['epochs'])) for e in epochs_range]\n",
    "\n",
    "ax7.plot(epochs_range, simulated_precision, marker='o', linewidth=2, markersize=8, \n",
    "         label='Precision', color='#3498db')\n",
    "ax7.plot(epochs_range, simulated_recall, marker='s', linewidth=2, markersize=8,\n",
    "         label='Recall', color='#e74c3c')\n",
    "ax7.axhline(y=0.85, color='green', linestyle='--', alpha=0.5, label='Target (0.85)')\n",
    "ax7.fill_between(epochs_range, simulated_precision, alpha=0.2, color='#3498db')\n",
    "ax7.set_xlabel('Epoch', fontweight='bold', fontsize=11)\n",
    "ax7.set_ylabel('Score', fontweight='bold', fontsize=11)\n",
    "ax7.set_title('Training Progress (Simulated Curve)', fontweight='bold', fontsize=12)\n",
    "ax7.legend()\n",
    "ax7.grid(alpha=0.3)\n",
    "ax7.set_ylim(0, 1)\n",
    "\n",
    "plt.suptitle('Sentence Transformer Fine-tuning Results - Algerian Youth Events', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.savefig('training_results.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Visualization saved to: training_results.png\")\n",
    "\n",
    "# === SAVE RESULTS ===\n",
    "results = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"config\": CONFIG,\n",
    "    \"baseline\": {\n",
    "        \"precision\": float(baseline_metrics['precision']),\n",
    "        \"recall\": float(baseline_metrics['recall']),\n",
    "        \"f1\": float(baseline_metrics['f1'])\n",
    "    },\n",
    "    \"finetuned\": {\n",
    "        \"precision\": float(finetuned_metrics['precision']),\n",
    "        \"recall\": float(finetuned_metrics['recall']),\n",
    "        \"f1\": float(finetuned_metrics['f1'])\n",
    "    },\n",
    "    \"improvement\": {\n",
    "        \"precision_pct\": float(precision_improvement),\n",
    "        \"recall_pct\": float(recall_improvement),\n",
    "        \"f1_pct\": float(f1_improvement)\n",
    "    },\n",
    "    \"training_time_seconds\": float(training_time)\n",
    "}\n",
    "\n",
    "with open('training_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Results saved to: training_results.json\")\n",
    "\n",
    "# === FINAL SUMMARY ===\n",
    "print(\"\\n TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüéØ Fine-tuned model location: {CONFIG['output_path']}\")\n",
    "print(f\"\\n üìä Metrics improved: {precision_improvement > 0 and recall_improvement > 0}\")\n",
    "print(f\"\\n ‚è±Ô∏è  Training time: {training_time:.1f}s\")\n",
    "print(f\"\\nüí° To use the fine-tuned model:\")\n",
    "print(f\" \\n   model = SentenceTransformer('{CONFIG['output_path']}')\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
